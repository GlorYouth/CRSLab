# @Time   : 2020/11/22
# @Author : Kun Zhou
# @Email  : francis_kun_zhou@163.com

# UPDATE:
# @Time   : 2020/11/24, 2020/12/29, 2021/1/4
# @Author : Kun Zhou, Xiaolei Wang, Yuanhang Zhou
# @Email  : francis_kun_zhou@163.com, wxl1999@foxmail.com, sdzyh002@gmail.com

r"""
KGSF
====
References:
    Zhou, Kun, et al. `"Improving Conversational Recommender Systems via Knowledge Graph based Semantic Fusion."`_ in KDD 2020.

.. _`"Improving Conversational Recommender Systems via Knowledge Graph based Semantic Fusion."`:
   https://dl.acm.org/doi/abs/10.1145/3394486.3403143

"""

import os

import numpy as np
import torch
import torch.nn.functional as F
from loguru import logger
from torch import nn
from torch_geometric.nn import GCNConv, RGCNConv

from crslab.config import MODEL_PATH
from crslab.model.base import BaseModel
from crslab.model.utils.functions import edge_to_pyg_format
from crslab.model.utils.modules.attention import SelfAttentionSeq
from crslab.model.utils.modules.transformer import TransformerEncoder
from .modules import GateLayer, TransformerDecoderKG
from .resources import resources


class KGSFModel(BaseModel):
    """

    Attributes:
        vocab_size: A integer indicating the vocabulary size.
        pad_token_idx: A integer indicating the id of padding token.
        start_token_idx: A integer indicating the id of start token.
        end_token_idx: A integer indicating the id of end token.
        token_emb_dim: A integer indicating the dimension of token embedding layer.
        pretrain_embedding: A string indicating the path of pretrained embedding.
        n_word: A integer indicating the number of words.
        n_entity: A integer indicating the number of entities.
        pad_word_idx: A integer indicating the id of word padding.
        pad_entity_idx: A integer indicating the id of entity padding.
        num_bases: A integer indicating the number of bases.
        kg_emb_dim: A integer indicating the dimension of kg embedding.
        n_heads: A integer indicating the number of heads.
        n_layers: A integer indicating the number of layer.
        ffn_size: A integer indicating the size of ffn hidden.
        dropout: A float indicating the dropout rate.
        attention_dropout: A integer indicating the dropout rate of attention layer.
        relu_dropout: A integer indicating the dropout rate of relu layer.
        learn_positional_embeddings: A boolean indicating if we learn the positional embedding.
        embeddings_scale: A boolean indicating if we use the embeddings scale.
        reduction: A boolean indicating if we use the reduction.
        n_positions: A integer indicating the number of position.
        response_truncate = A integer indicating the longest length for response generation.
        pretrained_embedding: A string indicating the path of pretrained embedding.

    """

    def __init__(self, opt, device, vocab, side_data):
        """

        Args:
            opt (dict): A dictionary record the hyper parameters.
            device (torch.device): A variable indicating which device to place the data and model.
            vocab (dict): A dictionary record the vocabulary information.
            side_data (dict): A dictionary record the side data.

        """
        self.opt = opt  # Store opt to access it in build methods
        self.device = device
        self.gpu = opt.get("gpu", [-1])
        # vocab
        self.vocab_size = vocab['vocab_size']
        self.pad_token_idx = vocab['pad']
        self.start_token_idx = vocab['start']
        self.end_token_idx = vocab['end']
        self.token_emb_dim = opt['token_emb_dim']
        self.pretrained_embedding = side_data.get('embedding', None)
        # kg
        self.n_word = vocab['n_word']
        self.n_entity = vocab['n_entity']
        self.pad_word_idx = vocab['pad_word']
        self.pad_entity_idx = vocab['pad_entity']
        entity_kg = side_data['entity_kg']
        self.n_relation = entity_kg['n_relation']
        entity_edges = entity_kg['edge']
        self.entity_edge_idx, self.entity_edge_type = edge_to_pyg_format(entity_edges, 'RGCN')
        self.entity_edge_idx = self.entity_edge_idx.to(device)
        self.entity_edge_type = self.entity_edge_type.to(device)
        word_edges = side_data['word_kg']['edge']

        self.word_edges = edge_to_pyg_format(word_edges, 'GCN').to(device)

        self.num_bases = opt['num_bases']
        self.kg_emb_dim = opt['kg_emb_dim']
        # transformer
        self.n_heads = opt['n_heads']
        self.n_layers = opt['n_layers']
        self.ffn_size = opt['ffn_size']
        self.dropout = opt['dropout']
        self.attention_dropout = opt['attention_dropout']
        self.relu_dropout = opt['relu_dropout']
        self.learn_positional_embeddings = opt['learn_positional_embeddings']
        self.embeddings_scale = opt['embeddings_scale']
        self.reduction = opt['reduction']
        self.n_positions = opt['n_positions']
        self.response_truncate = opt.get('response_truncate', 20)
        # copy mask
        dataset = opt['dataset']
        dpath = os.path.join(MODEL_PATH, "kgsf", dataset)
        resource = resources[dataset]
        super(KGSFModel, self).__init__(opt, device, dpath, resource)

    def build_model(self):
        self._init_embeddings()
        self._build_kg_layer()
        self._build_infomax_layer()
        self._build_recommendation_layer()
        self._build_conversation_layer()

    def _init_embeddings(self):
        if self.pretrained_embedding is not None:
            self.token_embedding = nn.Embedding.from_pretrained(
                torch.as_tensor(self.pretrained_embedding, dtype=torch.float), freeze=False,
                padding_idx=self.pad_token_idx)
        else:
            self.token_embedding = nn.Embedding(self.vocab_size, self.token_emb_dim, self.pad_token_idx)
            nn.init.normal_(self.token_embedding.weight, mean=0, std=self.kg_emb_dim ** -0.5)
            nn.init.constant_(self.token_embedding.weight[self.pad_token_idx], 0)

        self.word_kg_embedding = nn.Embedding(self.n_word, self.kg_emb_dim, self.pad_word_idx)
        nn.init.normal_(self.word_kg_embedding.weight, mean=0, std=self.kg_emb_dim ** -0.5)
        nn.init.constant_(self.word_kg_embedding.weight[self.pad_word_idx], 0)

        logger.debug('[Finish init embeddings]')

    def _build_kg_layer(self):
        # db encoder
        self.entity_encoder = RGCNConv(self.n_entity, self.kg_emb_dim, self.n_relation, self.num_bases)
        self.entity_self_attn = SelfAttentionSeq(self.kg_emb_dim, self.kg_emb_dim)

        # concept encoder
        self.word_encoder = GCNConv(self.kg_emb_dim, self.kg_emb_dim)
        self.word_self_attn = SelfAttentionSeq(self.kg_emb_dim, self.kg_emb_dim)

        # gate mechanism
        self.gate_layer = GateLayer(self.kg_emb_dim)

        logger.debug('[Finish build kg layer]')

    def _build_infomax_layer(self):
        self.infomax_norm = nn.Linear(self.kg_emb_dim, self.kg_emb_dim)
        self.infomax_bias = nn.Linear(self.kg_emb_dim, self.n_entity)
        self.infomax_loss = nn.MSELoss(reduction='sum')

        logger.debug('[Finish build infomax layer]')

    def _build_recommendation_layer(self):
        self.rec_bias = nn.Linear(self.kg_emb_dim, self.n_entity)
        # Get label_smoothing factor for recommendation loss
        # It might be in opt['rec_optim_opt']['label_smoothing'] or opt['label_smoothing_factor']
        # We'll try to get it from a general location first, then specific.
        rec_label_smoothing = self.opt.get('label_smoothing_factor', 0.0) # Try global first
        if 'rec_optim_opt' in self.opt and 'label_smoothing' in self.opt['rec_optim_opt']:
             rec_label_smoothing = self.opt['rec_optim_opt'].get('label_smoothing', rec_label_smoothing)
        elif 'label_smoothing' in self.opt : # Check if it's directly in opt for rec
            rec_label_smoothing = self.opt.get('label_smoothing', rec_label_smoothing)


        logger.info(f"Recommendation Label Smoothing Factor: {rec_label_smoothing}")
        self.rec_loss = nn.CrossEntropyLoss(label_smoothing=rec_label_smoothing)

        logger.debug('[Finish build rec layer]')

    def _build_conversation_layer(self):
        self.register_buffer('START', torch.tensor([self.start_token_idx], dtype=torch.long))
        self.conv_encoder = TransformerEncoder(
            n_heads=self.n_heads,
            n_layers=self.n_layers,
            embedding_size=self.token_emb_dim,
            ffn_size=self.ffn_size,
            vocabulary_size=self.vocab_size,
            embedding=self.token_embedding,
            dropout=self.dropout,
            attention_dropout=self.attention_dropout,
            relu_dropout=self.relu_dropout,
            padding_idx=self.pad_token_idx,
            learn_positional_embeddings=self.learn_positional_embeddings,
            embeddings_scale=self.embeddings_scale,
            reduction=self.reduction,
            n_positions=self.n_positions,
        )

        self.conv_entity_norm = nn.Linear(self.kg_emb_dim, self.ffn_size)
        self.conv_entity_attn_norm = nn.Linear(self.kg_emb_dim, self.ffn_size)
        self.conv_word_norm = nn.Linear(self.kg_emb_dim, self.ffn_size)
        self.conv_word_attn_norm = nn.Linear(self.kg_emb_dim, self.ffn_size)

        self.copy_norm = nn.Linear(self.ffn_size * 3, self.token_emb_dim)
        self.copy_output = nn.Linear(self.token_emb_dim, self.vocab_size)
        self.copy_mask = torch.as_tensor(np.load(os.path.join(self.dpath, "copy_mask.npy")).astype(bool),
                                         ).to(self.device)

        self.conv_decoder = TransformerDecoderKG(
            self.n_heads, self.n_layers, self.token_emb_dim, self.ffn_size, self.vocab_size,
            embedding=self.token_embedding,
            dropout=self.dropout,
            attention_dropout=self.attention_dropout,
            relu_dropout=self.relu_dropout,
            embeddings_scale=self.embeddings_scale,
            learn_positional_embeddings=self.learn_positional_embeddings,
            padding_idx=self.pad_token_idx,
            n_positions=self.n_positions
        )
        # Get label_smoothing factor for conversation loss
        conv_label_smoothing = self.opt.get('label_smoothing_factor', 0.0) # Try global first
        if 'conv_optim_opt' in self.opt and 'label_smoothing' in self.opt['conv_optim_opt']:
            conv_label_smoothing = self.opt['conv_optim_opt'].get('label_smoothing', conv_label_smoothing)
        elif 'label_smoothing' in self.opt : # Check if it's directly in opt for conv
            conv_label_smoothing = self.opt.get('label_smoothing', conv_label_smoothing)

        logger.info(f"Conversation Label Smoothing Factor: {conv_label_smoothing}")
        self.conv_loss = nn.CrossEntropyLoss(ignore_index=self.pad_token_idx, label_smoothing=conv_label_smoothing)

        logger.debug('[Finish build conv layer]')

    def pretrain_infomax(self, batch):
        """
        words: (batch_size, word_length)
        entity_labels: (batch_size, n_entity)
        """
        words, entity_labels = batch

        loss_mask = torch.sum(entity_labels)
        if loss_mask.item() == 0:
            return None

        entity_graph_representations = self.entity_encoder(None, self.entity_edge_idx, self.entity_edge_type)
        word_graph_representations = self.word_encoder(self.word_kg_embedding.weight, self.word_edges)

        word_representations = word_graph_representations[words]
        word_padding_mask = words.eq(self.pad_word_idx)  # (bs, seq_len)

        word_attn_rep = self.word_self_attn(word_representations, word_padding_mask)
        word_info_rep = self.infomax_norm(word_attn_rep)  # (bs, dim)
        info_predict = F.linear(word_info_rep, entity_graph_representations, self.infomax_bias.bias)  # (bs, #entity)
        loss = self.infomax_loss(info_predict, entity_labels) / loss_mask
        return loss

    def recommend(self, batch, mode):
        """
        context_entities: (batch_size, entity_length)
        context_words: (batch_size, word_length)
        movie: (batch_size)
        """
        context_entities, context_words, entities, movie = batch

        entity_graph_representations = self.entity_encoder(None, self.entity_edge_idx, self.entity_edge_type)
        word_graph_representations = self.word_encoder(self.word_kg_embedding.weight, self.word_edges)

        entity_padding_mask = context_entities.eq(self.pad_entity_idx)  # (bs, entity_len)
        word_padding_mask = context_words.eq(self.pad_word_idx)  # (bs, word_len)

        entity_representations = entity_graph_representations[context_entities]
        word_representations = word_graph_representations[context_words]

        entity_attn_rep = self.entity_self_attn(entity_representations, entity_padding_mask)
        word_attn_rep = self.word_self_attn(word_representations, word_padding_mask)

        user_rep = self.gate_layer(entity_attn_rep, word_attn_rep)
        rec_scores = F.linear(user_rep, entity_graph_representations, self.rec_bias.bias)  # (bs, #entity)

        rec_loss = self.rec_loss(rec_scores, movie)

        info_loss_mask = torch.sum(entities)
        if info_loss_mask.item() == 0:
            info_loss = None
        else:
            word_info_rep = self.infomax_norm(word_attn_rep)  # (bs, dim)
            info_predict = F.linear(word_info_rep, entity_graph_representations,
                                    self.infomax_bias.bias)  # (bs, #entity)
            info_loss = self.infomax_loss(info_predict, entities) / info_loss_mask

        return rec_loss, info_loss, rec_scores

    def freeze_parameters(self):
        freeze_models = [self.word_kg_embedding, self.entity_encoder, self.entity_self_attn, self.word_encoder,
                         self.word_self_attn, self.gate_layer, self.infomax_bias, self.infomax_norm, self.rec_bias]
        for model in freeze_models:
            for p in model.parameters():
                p.requires_grad = False

    def _starts(self, batch_size):
        """Return bsz start tokens."""
        return self.START.detach().expand(batch_size, 1)

    def _decode_forced_with_kg(self, token_encoding, entity_reps, entity_emb_attn, entity_mask,
                               word_reps, word_emb_attn, word_mask, response):
        batch_size, seq_len = response.shape
        start = self._starts(batch_size)
        inputs = torch.cat((start, response[:, :-1]), dim=-1).long()

        dialog_latent, _ = self.conv_decoder(inputs, token_encoding, word_reps, word_mask,
                                             entity_reps, entity_mask)  # (bs, seq_len, dim)
        entity_latent = entity_emb_attn.unsqueeze(1).expand(-1, seq_len, -1)
        word_latent = word_emb_attn.unsqueeze(1).expand(-1, seq_len, -1)
        copy_latent = self.copy_norm(
            torch.cat((entity_latent, word_latent, dialog_latent), dim=-1))  # (bs, seq_len, dim)

        copy_logits = self.copy_output(copy_latent) * self.copy_mask.unsqueeze(0).unsqueeze(
            0)  # (bs, seq_len, vocab_size)
        gen_logits = F.linear(dialog_latent, self.token_embedding.weight)  # (bs, seq_len, vocab_size)
        sum_logits = copy_logits + gen_logits
        preds = sum_logits.argmax(dim=-1)
        return sum_logits, preds

    def _decode_greedy_with_kg(self, token_encoding, entity_reps, entity_emb_attn, entity_mask,
                               word_reps, word_emb_attn, word_mask):
        batch_size = token_encoding[0].shape[0]
        inputs = self._starts(batch_size).long()
        incr_state = None
        logits = []
        for _ in range(self.response_truncate):
            dialog_latent, incr_state = self.conv_decoder(inputs, token_encoding, word_reps, word_mask,
                                                          entity_reps, entity_mask, incr_state)
            dialog_latent = dialog_latent[:, -1:, :]  # (bs, 1, dim)
            db_latent = entity_emb_attn.unsqueeze(1)
            concept_latent = word_emb_attn.unsqueeze(1)
            copy_latent = self.copy_norm(torch.cat((db_latent, concept_latent, dialog_latent), dim=-1))

            copy_logits = self.copy_output(copy_latent) * self.copy_mask.unsqueeze(0).unsqueeze(0)
            gen_logits = F.linear(dialog_latent, self.token_embedding.weight)
            sum_logits = copy_logits + gen_logits
            preds = sum_logits.argmax(dim=-1).long()
            logits.append(sum_logits)
            inputs = torch.cat((inputs, preds), dim=1)

            finished = ((inputs == self.end_token_idx).sum(dim=-1) > 0).sum().item() == batch_size
            if finished:
                break
        logits = torch.cat(logits, dim=1)
        return logits, inputs

    def _decode_beam_search_with_kg(self, token_encoding, entity_reps, entity_emb_attn, entity_mask,
                                    word_reps, word_emb_attn, word_mask, beam=4):
        batch_size = token_encoding[0].shape[0]
        inputs = self._starts(batch_size).long().reshape(1, batch_size, -1)
        incr_state = None

        sequences = [[[list(), list(), 1.0]]] * batch_size
        for i in range(self.response_truncate):
            if i == 1:
                token_encoding = (token_encoding[0].repeat(beam, 1, 1),
                                  token_encoding[1].repeat(beam, 1, 1))
                entity_reps = entity_reps.repeat(beam, 1, 1)
                entity_emb_attn = entity_emb_attn.repeat(beam, 1)
                entity_mask = entity_mask.repeat(beam, 1)
                word_reps = word_reps.repeat(beam, 1, 1)
                word_emb_attn = word_emb_attn.repeat(beam, 1)
                word_mask = word_mask.repeat(beam, 1)

            # at beginning there is 1 candidate, when i!=0 there are 4 candidates
            if i != 0:
                inputs = []
                for d in range(len(sequences[0])):
                    for j in range(batch_size):
                        text = sequences[j][d][0]
                        inputs.append(text)
                inputs = torch.stack(inputs).reshape(beam, batch_size, -1)  # (beam, batch_size, _)

            with torch.no_grad():
                dialog_latent, incr_state = self.conv_decoder(
                    inputs.reshape(len(sequences[0]) * batch_size, -1),
                    token_encoding, word_reps, word_mask,
                    entity_reps, entity_mask, incr_state
                )
                dialog_latent = dialog_latent[:, -1:, :]  # (bs, 1, dim)
                db_latent = entity_emb_attn.unsqueeze(1)
                concept_latent = word_emb_attn.unsqueeze(1)
                copy_latent = self.copy_norm(torch.cat((db_latent, concept_latent, dialog_latent), dim=-1))

                copy_logits = self.copy_output(copy_latent) * self.copy_mask.unsqueeze(0).unsqueeze(0)
                gen_logits = F.linear(dialog_latent, self.token_embedding.weight)
                sum_logits = copy_logits + gen_logits

            logits = sum_logits.reshape(len(sequences[0]), batch_size, 1, -1)
            # turn into probabilities,in case of negative numbers
            # Note: Softmax is applied here for beam search scoring, not directly part of loss calculation
            probs, preds = torch.nn.functional.softmax(logits, dim=-1).topk(beam, dim=-1)


            # (candeidate, bs, 1 , beam) during first loop, candidate=1, otherwise candidate=beam

            for j in range(batch_size):
                all_candidates = []
                for n in range(len(sequences[j])):
                    for k in range(beam):
                        prob = sequences[j][n][2]
                        logit_history = sequences[j][n][1] # Renamed for clarity
                        current_input_sequence = inputs[n][j].reshape(-1)
                        predicted_token = preds[n][j][0][k].reshape(-1)
                        
                        # For the first step, logit_history is empty
                        if not isinstance(logit_history, torch.Tensor) and not logit_history: # Check if empty list
                            logit_tmp = logits[n][j][0].unsqueeze(0)
                        else:
                            logit_tmp = torch.cat((logit_history, logits[n][j][0].unsqueeze(0)), dim=0)
                        
                        seq_tmp = torch.cat((current_input_sequence, predicted_token))
                        candidate = [seq_tmp, logit_tmp, prob * probs[n][j][0][k].item()] # Use .item() for scalar prob
                        all_candidates.append(candidate)
                ordered = sorted(all_candidates, key=lambda tup: tup[2], reverse=True)
                sequences[j] = ordered[:beam]

            # check if everyone has generated an end token
            # We need to check the sequences generated so far
            all_finished_this_beam = True
            current_inputs_for_check = torch.stack([s[0][0] for s in sequences]) # Get current sequences from beam
            if current_inputs_for_check.numel() > 0 : # Ensure tensor is not empty
                 all_finished = ((current_inputs_for_check == self.end_token_idx).sum(dim=-1) > 0).all().item() # Check if all sequences in beam have END
                 if all_finished:
                    break
            else: # Should not happen if batch_size > 0
                break


        # Prepare final output based on best sequences
        final_logits_list = []
        final_inputs_list = []
        for j in range(batch_size):
            # sequences[j][0] should be the best sequence for this batch item
            if sequences[j]: # Check if sequences[j] is not empty
                 final_inputs_list.append(sequences[j][0][0])
                 if isinstance(sequences[j][0][1], torch.Tensor): # Check if logits were accumulated
                    final_logits_list.append(sequences[j][0][1])
                 else: # Handle case where no logits were accumulated (e.g. very short generation)
                    # Create a dummy tensor or handle as appropriate for your logic
                    # For now, let's assume we need to provide something of expected shape if possible
                    # This part might need adjustment based on how downstream code uses these logits
                    # If only preds are used, this might be less critical.
                    # Let's try to create a placeholder if empty, otherwise it will fail torch.stack
                    # This is a tricky part if generation is shorter than expected or beam search terminates early.
                    # For simplicity, if no logits, we might not be able to stack.
                    # Consider what the expected output format is if generation is very short.
                    # For now, if no logits, we will skip adding to final_logits_list,
                    # and this will cause an error in torch.stack if final_logits_list is empty.
                    # A more robust solution would be to pad or ensure consistent output.
                    pass # Or append a tensor of zeros with expected shape if needed

        if not final_inputs_list: # If all sequences were empty for some reason
            # Fallback or error handling
            # This case should ideally not be reached if batch_size > 0
            # Returning empty tensors or raising an error might be options
            return torch.empty(0), torch.empty(0, dtype=torch.long)


        inputs = torch.stack(final_inputs_list)
        # Only stack logits if the list is not empty and elements are tensors
        if final_logits_list and all(isinstance(lg, torch.Tensor) for lg in final_logits_list):
            try:
                # Ensure all logit tensors have the same sequence length for stacking
                # This might require padding if sequences have different lengths,
                # but beam search typically generates up to response_truncate or END token.
                # If lengths differ, this will error.
                # For now, assume lengths are consistent from beam search logic or handle padding if necessary.
                # The original code `torch.stack([seq[0][1] for seq in sequences])` assumed this.
                max_len = max(lg.shape[0] for lg in final_logits_list)
                padded_logits = []
                for lg in final_logits_list:
                    pad_len = max_len - lg.shape[0]
                    if pad_len > 0:
                        # Pad with a value that won't affect softmax much if it's further processed, e.g., -inf or 0
                        # Or, ensure beam search outputs consistent lengths or handle this padding more carefully.
                        # For CrossEntropy, padding value in logits doesn't matter if target is ignore_index.
                        # Here, these are intermediate logits from beam search.
                        padding = torch.zeros((pad_len, lg.shape[1]), device=lg.device, dtype=lg.dtype)
                        padded_logits.append(torch.cat((lg, padding), dim=0))
                    else:
                        padded_logits.append(lg)
                logits = torch.stack(padded_logits)
            except Exception as e:
                logger.error(f"Error stacking logits in beam search: {e}")
                logger.error(f"Logits list: {[lg.shape for lg in final_logits_list if isinstance(lg, torch.Tensor)]}")
                # Fallback: return inputs and potentially empty or error-indicating logits
                return torch.empty(0, device=inputs.device), inputs

        elif not final_logits_list: # If list is empty (e.g. all generations were too short)
             logits = torch.empty(0, device=inputs.device) # Or handle as an error
        else: # Mixed types or other issues
            logger.error("final_logits_list contains non-tensor elements or is problematic.")
            logits = torch.empty(0, device=inputs.device)


        return logits, inputs


    def converse(self, batch, mode):
        context_tokens, context_entities, context_words, response = batch

        entity_graph_representations = self.entity_encoder(None, self.entity_edge_idx, self.entity_edge_type)
        word_graph_representations = self.word_encoder(self.word_kg_embedding.weight, self.word_edges)

        entity_padding_mask = context_entities.eq(self.pad_entity_idx)  # (bs, entity_len)
        word_padding_mask = context_words.eq(self.pad_word_idx)  # (bs, seq_len)

        entity_representations = entity_graph_representations[context_entities]
        word_representations = word_graph_representations[context_words]

        entity_attn_rep = self.entity_self_attn(entity_representations, entity_padding_mask)
        word_attn_rep = self.word_self_attn(word_representations, word_padding_mask)

        # encoder-decoder
        tokens_encoding = self.conv_encoder(context_tokens)
        conv_entity_emb = self.conv_entity_attn_norm(entity_attn_rep)
        conv_word_emb = self.conv_word_attn_norm(word_attn_rep)
        conv_entity_reps = self.conv_entity_norm(entity_representations)
        conv_word_reps = self.conv_word_norm(word_representations)
        if mode != 'test':
            logits, preds = self._decode_forced_with_kg(tokens_encoding, conv_entity_reps, conv_entity_emb,
                                                        entity_padding_mask,
                                                        conv_word_reps, conv_word_emb, word_padding_mask,
                                                        response)

            logits = logits.view(-1, logits.shape[-1])
            response = response.view(-1)
            loss = self.conv_loss(logits, response)
            return loss, preds
        else: # mode == 'test'
            # In test mode, you might want to use greedy or beam search
            # The original code used _decode_greedy_with_kg for 'test'
            # If beam search is preferred for testing:
            # logits, preds = self._decode_beam_search_with_kg(tokens_encoding, conv_entity_reps, conv_entity_emb,
            #                                             entity_padding_mask,
            #                                             conv_word_reps, conv_word_emb, word_padding_mask,
            #                                             beam=self.opt.get('beam_size', 4)) # Get beam_size from opt
            # For now, sticking to original greedy for 'test' as per provided code structure
            _, preds = self._decode_greedy_with_kg(tokens_encoding, conv_entity_reps, conv_entity_emb,
                                                      entity_padding_mask,
                                                      conv_word_reps, conv_word_emb, word_padding_mask)
            return preds # In test mode, typically only predictions are returned, not loss or raw logits

    def forward(self, batch, stage, mode):
        if len(self.gpu) >= 2:
            #  forward function operates on different gpus, the weight of graph network need to be copied to other gpu
            current_device = torch.cuda.current_device()
            self.entity_edge_idx = self.entity_edge_idx.to(current_device)
            self.entity_edge_type = self.entity_edge_type.to(current_device)
            self.word_edges = self.word_edges.to(current_device)
            # Re-load or ensure copy_mask is on the correct device if it's not a registered buffer
            # If it's frequently reloaded, consider making it a buffer if its content is static per dataset
            self.copy_mask = torch.as_tensor(np.load(os.path.join(self.dpath, "copy_mask.npy")).astype(bool),
                                             dtype=torch.bool).to(current_device) # Ensure correct dtype for boolean mask
        
        output = None # Initialize output
        if stage == "pretrain":
            output = self.pretrain_infomax(batch)
        elif stage == "rec":
            # recommend returns rec_loss, info_loss, rec_scores
            # The system likely expects a single loss or a tuple that it can unpack
            rec_loss, info_loss, rec_scores = self.recommend(batch, mode)
            # How these losses are combined or handled depends on the system's training loop
            # For now, let's assume the system primarily uses rec_loss for optimization if only one loss is expected
            # Or, it might handle a dictionary of losses.
            # Returning a tuple, the system might take the first element as the primary loss.
            if info_loss is not None:
                output = (rec_loss + info_loss, rec_scores) # Example: combine losses
            else:
                output = (rec_loss, rec_scores)

        elif stage == "conv":
            if mode != 'test':
                loss, preds = self.converse(batch, mode)
                output = (loss, preds)
            else:
                preds = self.converse(batch, mode)
                output = preds # In test mode for converse, only preds are returned
        return output

