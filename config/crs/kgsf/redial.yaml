# dataset
dataset: ReDial
tokenize: nltk
embedding: word2vec.npy
# dataloader
context_truncate: 256
response_truncate: 30
scale: 1
# model
model: KGSF
token_emb_dim: 300
kg_emb_dim: 128
num_bases: 8
n_heads: 2
n_layers: 2
ffn_size: 300
dropout: 0.1
attention_dropout: 0.0
relu_dropout: 0.1
learn_positional_embeddings: false
embeddings_scale: true
reduction: false
n_positions: 1024
# optim
pretrain:
  epoch: 4
  batch_size: 128
  optimizer:
    name: Adam
    lr: !!float 5e-5  # 初始学习率 (initial_lr)
  lr_scheduler:
    name: LinearIncreasingLR # 使用新定义的调度器
    total_epochs: 3 # 对应 pretrain 阶段的总 epoch 数
    max_lr: !!float 1e-3 # 在 pretrain 阶段结束时希望达到的最大学习率
    warmup_steps: 0 # 如果不需要预热，可以设为0；否则例如设置为 train_dataloader_len * 1 (一个epoch的步数)
rec:
  epoch: 3
  batch_size: 128
  optimizer:
    name: Adam
    lr: !!float 1e-3
conv:
  epoch: 3
  batch_size: 128
  optimizer:
    name: Adam
    lr: !!float 1e-3
  lr_scheduler:
    name: ReduceLROnPlateau
    patience: 3
    factor: 0.5
  gradient_clip: 0.1
